https://developer.confluent.io/courses/flink-java/overview/

Datastream programming:
    Process the records as they arrive
    The event pushed to a series of Operators connected to a DAG
    Each operator perform a transformation and out put the event

    Manage state:
        Events might interconnected so we have to keep the state
        Flink help us to define statefull operations

Flink job lifecycle:

    A unit of work in flink called job
    A job can be written to SQL,Python, JAVA
    If is Java then the job will packaged to a jar file and deployed to Flink Cluster

    A job has one or many Operations

    Java job:

        Create a job:
            1. The job entry point is the main (public static void main) method of the project
            2. The job need the StreamExecutionEnvironment object 
            3. The job defines the steps of the stream

            The main method is registered as entry point in the manifest file 
        Run the job:

               flink run <jar file path> 

            if entry point didnt declared in the manifest we can execute:
                my.package.MyClass: class contains the main method
                flink run -c my.package.MyClass <jar file path>

            --detached: run the job in detach mode 

            Cancel a job:
                <job id>: will be return from flink run command
                flink cancel <job id>

            Stop job:
                --savepointPath: where to store the state in order to resume later
                flink stop --savepointPath <folder path> <job id>
            
            Resuming a job:
                --fromSavePoint: resume from saved state 
                flink run --fromSavePoint <folder path> <job id>
        Life cycle:
            Run --->
            Created: the cluster seen the job but didnt allocated resources yet
            Schedule ->
            Running: job manager assign resources (allocate tasks slots) to the job
            
            Finish -> if its not operating in infinite source and consumed all elements 
            Canceled: we have run the Cancele comand to terminate a continusly job
                    the job cannot resume, i can initiate a new instance
            Suspended: after run the stop command
            Created: when resume a job it will go back to Created and wit for schedule

            Failing: if failed and the reason is not restartable
            Failed: failed

    Restart job:

        Strategy: 
            there is global configuration or can be set at job level using the setRestartStrategy
                fixed-delay
                failure-rate
                exponential-delay

Flink job anatomy:

    Data Source: source of data (kafka topic,log files, databases..)
                 finite data source: if we consume all events from the source then the stream will close
                 infinite data source: continue to consuming as long the data source exists
                
                 Methods:
                    for testing:
                        fromElements: to read from a finite source
                        DataGeneratorSource: each time requested data it will send a record based on a function
                            (
                                index -> <function>
                                numRecords: limit on how many rows to send
                                rate limit per secodn
                                type: data type
                            )
                    prod sources:

                        FileSource: stream records one at a time
                            (
                                Line format
                                path of the file
                            )

                        KafkaSoruce:
                            (
                                 properties
                                ,topics list
                                ,deserializer
                            )

    Operators:  transform the stream (filter, redirect, aggregate, branch points, ..)
    Datastream Backpressure: if one operator of the stream is slow, flink will notify the upstream operators until the source to not send more data 
    Sink: destination 
    Stream:
        DataStream<Object> stream = env.fromSource(
            source
            watermark strategy
            name of source
        )


Serialization and Deserialization:

    External Serial/Deserial: when mesage comes from source or pushed to sink

            JsonSerializationSchema<T>: serialize from java obkect to json
            JsonDeserializationSchema<>(T.class): deserialize json to Java object

            For some objects like JavaTimeModule that does not support serializetion, we have to createa ObjectMapper and register the Modulewith serializer
    Internal: object transfered between Operators

        If class comply with the rules of POJO then flink can serialize it more efficient
        POJO: Plain Old Java Objects
        Rules:
            1. Class should be Public
            2. Must have a parameter less default constructor
            3. All fields must be public or accessible from gettters, setters methods
            4. All fields in the class must be supported by an appropriate serializer

            If class meet the criteria then flink wil use the faster POJO serializer
            Otherwise will use the slower KRYO serializer 

            iF KRYO serializer will be used then we can register Types with the serializer
            I can also disable the kryo serializer but i have to maek sure that the Types support POJO 
            Other than that Avro serializer is another good choise

            Using POJO serializer we can support Schema Evolution


        POJO should be simple, lightweight and perhaps implement basic methods like  toString(), equals(), and hashCode()

Transformations:
    1. Define the Process function: takes an input and convert it to output

            public class MyProcessFunction extends ProcessFunction<Input, Output>{

                @Override
                public void processElement(
                     input: input object
                    ,contxt: provides helper methods and objects
                    ,colelctor: collection of zero or more output objects
                ){}


                In process function we can:
                    1. Just to convert the Input object to output object
                    2. Filter out the input object and not return Output object
                    3. Based on one Input object to produce multiple Output objects

                    Operations to transform:
                        Map: based on one input produce a new output
                        Flatmap: based on one input produce multiple outputs
                        Filter: filter teh input based on condition
                        keyBy:  partition the stream, each input might redirected to different task manager (shuffle)
                        reduce: take the current input and combine it with previous element using a function 

    2. Attach the process function to the stream

            stream.process(new ProcessFunction())


Sink:

    1. Create a serializer for the output message

            For kafka:
                KafkaRecordSerializer<T>builder()
                .setTopic("topic-name")
                .setValueSerializationSchema()
                .build()

    2. Create the kafka sink

            For kafka:
                KafkaSink<T> sink = KafkaSink.<T>builder()
                                    .setKafkaProducerConfig(conf)
                                    .setRecordSerializer(serializer)
                                    .setDeliveryGuarantee(DeliveryGuarantee.NONE) // NONE, AT_LEAST_ONCE, EXACTLY_ONCE 

    3. Attach to stream

            toSink(sink)


Branch a stream:
    
        Fan-out: from the input split to multiple output
                1. By assigning the operations on the stream to separate object we can sink each stream to sepatrate target sink 

                    Datastream<T> out1 = stream.filter(...)
                    Datastream<T> out2 = stream.filter(...).map()

                2. Using Side outputs

        Fan-in:  multiple inputs combined in one stream
                 
                Union:
                      Takes two streams of the SAME DATA TYPE and union them into one stream
                      union = stream1.union(streams2)

                Connect:
                     Takes two streams of DIFFERENT DATA TYPES and connect them to one
                     connectstream = stream1.connect(stream2)

                    Then we have to declare the CoProcessFunction which contains two process functions
                    One for the input1 and one for the input2 objects
                    The functions can be CoMapFunction and CoFlatMapFunction 
                        connectstream.process(new CoProcessFunction<Inp1, Inp2, Output>)

                Union vs Connect:

                    The main difference is when you Map the objects to the new Type
                    For Union the Map is before the Union operator
                    For Connect we Map the object to the new type after the connect operator 

                    For simple stateless operation is better to  apply a Map and convert to new type and then apply Union
                    For cases where the state must be maintain between the two stream we use CoProcess