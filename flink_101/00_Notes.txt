APIs: in SQL, Python, Java
Streaming: unbounded stream of events


State
Time
Snapshots


Job graph (Topology):  
    Running a flink application is called job
    Events are passing throught a data processing pipeline called Job Graph (DAG)
    Each processing step of the Graph called Node
    Each Node of the Graph executed by an Operator
    Operators transforme the events


    A stream is partitioned to parallel substreams 
    Each substream can independendly prosessed

    An operator Forward the events to the substreams based on partition logic
    Another operator can filter the events of the partiotn based on other logic

    Events at some point might be shuffled or repartitioned to colocate the events in the same partition
    On shuffle or repartition the events must be serialized, moved over the network and go to other Node

    If we decide to change the partition logic then we have a Rebalance operation (works like shuffle)

    We can also Broadcast a partition to distribute it to all the Nodes
    We can Join streams to enrich the data


FlinkSQL:

    Can be used for both batch and streaming
        
    Flink abstraction table: 
                    FlinkSQL (Higher lelvel of abstraction)     simple sql to process streams 
                Table API(dynamic tables)                       declarative DSL (java or python code)
            DataStream API(streams, windows)                    stream processing and analytics
        Process functions (events, state, time)                 low level stateful stream processing


    Table: describe data that exists somewhere else
           Schema: column names and data types
           Connection properties: connection, topic, value format, bootstrap server...
          

          Table durability: Append Only or Updating tables

                Table event types:
                    +I: insertion: default
                    -U: Update before: retract an earlier event
                    +U: Update after: update an earlier event
                    -D: Delete: delete an earlier event

                    Iinput stream (Append only)     Output stream (Updating table)
                    New record came proA 50:        +I 50
                    New record came proB 20:        +I 20
                    New record came proA -15:       -U 50, +U 35 (remove the row that shows 50 and add new row with 35)
    
    Batch vs Stream:

        Sorting:
            Batch: sort by anything
            Stream: can sort only by time asc
        Joins:
            Batch: INNER and Outer joins
            Stream: special joins with Temporal table or external lookup table

Flink runtime:  

    What happens when submitting a job:
        When executing code with an API the api becomes the Client
        When execute the client the API asseble the Job DAG and submit it to Job Manager
        When Job Manager receive a job will refine and create the resources to run the job
        The Job Manager will spinup Task Manager instances as needed to provide the desire parallelism
        Each Task manager provide some tasks slows
        Each Task slot can execute one parallel instace of the job Graph
        When the job running the Job manager will coordinate the 
                Check checkpointing 
                Restar task manager if they failed

        Task managers take the data from the source, transformed
        They can also exchange data between them for repartition or rebalance
        Finally Task managers sink the data to the output stream

    Streaming vs Batch:

        Stream support bounded and unbounded stream
        Batch support onlu bounded streams

        In stream the entire pipeline must always running
        In batch execution proceeds in stages running as needed

        In stream input must processed as it arrivews
        In batch may be pre-sorted by btime and key (to make faster joins..)

        In stream results are reported as they become ready
        In batch results are reportedat the end

        In stream failure recovery resumes from the recent snapshot
        In batch failure does a reset and full restart

        Flink guarantees exactly-one results despite out-of-order data and failures

Stateful Processing:

    WHERE, SELECT and transformations does not require state (STATELESS)
    Aggregations (GROUP BY), JOINS  are operations that require state
    Each Node maintains at a data store the state for each value which is responsible

    State: In streaming has to handle it (not in batch mode)
        Each Node store the state Local
        Periodically Flink checkpooint the state to remode durable storage (s3)

        If State has unbounded number of data at some point the data store will be full and flink job will fail

        Temporal (safely stateful): Use TUBLE in query to specify windowed aggregations
            time-windowd aggregations
            interval joins
            time-versioned joins
            MATCH_RECOGNIZE (pattern matching)
                

            
Event time and watermarks: 
    watermarks make sense Only when we use Event time
    Watermark is like report the event happened on a specific window

    Event time: the time the event originated to the source
    Process time: event arrived to the reltime processing engine

    When processing based on Event time we might have out of order data
    In order to decide when the final calculation of the window will be we use the out-of-orderness estimate 
    Each watermark is the max timestamp seen so far - out-of-orderness

    Flink watermerk generator insert watermarks in the event stream
    Watermark messasge is an assumption that stream processed completed up to but ton include timestamp seen so far - out-of-orderness
    Events with event time before watermark considered as late events (ignored be default, use Datastream API to handle them custom)
    We will finalise the window calculation after we see watermarks after the window end time

    Watermark generator runs inside Flink kafka consumer
    Every 200ms the Watermark generator will generate a watermark
    Watermark generator will calculate the watermark from each partition is consuming
    The watermark the generator producing is the minimum of the watermarks of each partition

    If a partition does not reported any events then the watermark will be No WM
    If i have a partition with No WM then consumer will report No WM for all its partitions
    No WM vs 1:05  ===> No WM (like null)


Checkpoint and Recovery:

    To recover from failure Flink takes Snapshots of the State
    Snapshots are stored on a durable storage like s3
        Checkpoint:  
            Are managed from Flink
            Used to recover in case of failure
            Format is optimize to speedup recovery
        Savepoint:  
            Are manual snapshots created for some operational purpose( upgrade to new version,deploy new version of app, ..)


    Each operator contribute to the checkpoint
        Source operator: write the offsets of the topics
        Filter: filter does not contribute to the checkpoint
        Count:  counters or each element
        Sink: ids of commited transactions


    In case of recovery the phole cluster will need to restart from the point of the most recent snapshot